{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_df_list: Datasets aus gegebenen Path laden und in Liste schreiben\n",
    "\n",
    "\n",
    "def create_df_list(path):\n",
    "    \n",
    "    # get a list of all CSV files in the directory\n",
    "    files = glob.glob(path + '/*.csv')\n",
    "\n",
    "    # create an empty list to store the DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # iterate over the list of files\n",
    "    for file in files:\n",
    "        # read in the CSV file using pandas\n",
    "        df = pd.read_csv(file)\n",
    "        # append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "    # returns the list of dataframes\n",
    "    return df_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastFD: generiert Kandidaten\n",
    "# gibt ein-elementige rechte Seite mit Attributen aus Liste und alle linken Seiten die min \"numberofAttributs\"-Elementig\n",
    "\n",
    "def fastFD(df, lenght_lhs, null_attributes):\n",
    "    \"\"\"\n",
    "    Finds functional dependencies in a DataFrame using the FastFD algorithm.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the set of FDs with the trivial ones\n",
    "    fds = set([(frozenset([attr]), frozenset([])) for attr in df.columns])\n",
    "    non_trivial_fds = set()\n",
    "\n",
    "    # Step 2: Compute the equivalence classes of the tuples\n",
    "    eq_classes = compute_equivalence_classes(df)\n",
    "\n",
    "    # Step 3: Compute the closure of each attribute set\n",
    "    closures = {}\n",
    "    for attr in df.columns:\n",
    "        closures[frozenset([attr])] = closure(set([attr]), fds)\n",
    "\n",
    "    # Step 4: Initialize the set of candidate pairs\n",
    "    candidate_pairs = set([(frozenset([A]), frozenset([B])) for A in df.columns for B in list(null_attributes) if A != B])\n",
    "\n",
    "    # Step 5: Repeat until there are no more FDs to be found\n",
    "    while len(candidate_pairs) > 0:\n",
    "        # Step 5a: Choose a pair of attributes (A, B) such that A -> B is not already known\n",
    "        (A, B) = candidate_pairs.pop()\n",
    "\n",
    "        # Step 5b: Compute A+\n",
    "        AB_union = A.union(B)\n",
    "        AB_closure = closures[A].intersection(closures[B])\n",
    "        for attr in df.columns:\n",
    "            AB_attr = AB_union.union(set([attr]))\n",
    "            if AB_attr not in closures:\n",
    "                closures[AB_attr] = closure(AB_attr, fds)\n",
    "\n",
    "        # Step 5c: If B is in A+, add A -> B to the set of known FDs\n",
    "        if B not in AB_closure:\n",
    "            fds.add((A, B))\n",
    "            non_trivial_fds.add((A, B))\n",
    "\n",
    "            # Update the closures of all supersets of A\n",
    "            for attr_set in [s for s in closures.keys() if A.issubset(s) and s != A]:\n",
    "                closures[attr_set] = closures[attr_set].intersection(AB_closure)\n",
    "\n",
    "            # Update the set of candidate pairs\n",
    "            for C in null_attributes:\n",
    "                if C not in AB_union:\n",
    "                    candidate_pairs.add((AB_union, frozenset([C])))\n",
    "\n",
    "    # Step 6: Return the set of known FDs\n",
    "    result = [(list(X), list(Y)) for (X, Y) in fds if Y and len(X) <= lenght_lhs]\n",
    "    return result\n",
    "\n",
    "def compute_equivalence_classes(df):\n",
    "    \"\"\"\n",
    "    Computes the equivalence classes of the tuples in the DataFrame.\n",
    "    \"\"\"\n",
    "    eq_classes = defaultdict(set)\n",
    "    for row in df.itertuples(index=False):\n",
    "        eq_classes[row] = set(row)\n",
    "    return eq_classes\n",
    "\n",
    "def closure(X, fds):\n",
    "    \"\"\"\n",
    "    Computes the closure of a set of attributes X given a set of FDs.\n",
    "    \"\"\"\n",
    "    X_closure = set(X)\n",
    "    while True:\n",
    "        changed = False\n",
    "        for (A, B) in fds:\n",
    "            if A.issubset(X_closure) and not B.issubset(X_closure):\n",
    "                X_closure = X_closure.union(B)\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "    return X_closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_FDs: verifiziert Kandidaten\n",
    "\n",
    "def verify_FDs(df, candidates):\n",
    "    fds = []\n",
    "    for lhs, rhs in candidates:\n",
    "        if (df.groupby(list(lhs))[list(rhs)].nunique().eq(1).all() == True).all():\n",
    "            fds.append((set(lhs), set(rhs)))\n",
    "\n",
    "    # Remove any candidate pairs where there is another candidate pair with the same RHS and a larger LHS\n",
    "    pruned_fds = []\n",
    "    for lhs1, rhs1 in fds:\n",
    "        is_superset = False\n",
    "        for lhs2, rhs2 in fds:\n",
    "            if lhs1.issuperset(lhs2) and rhs1 == rhs2 and lhs1 != lhs2:\n",
    "                is_superset = True\n",
    "                break\n",
    "        if not is_superset:\n",
    "            pruned_fds.append((lhs1, rhs1))\n",
    "\n",
    "    return pruned_fds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_FDs: gibt zu Dataframe FDs zurÃ¼ck\n",
    "\n",
    "def find_FDs(df):\n",
    "    null_attributes = df.columns[df.isnull().any()].tolist()\n",
    "    candidates = fastFD(df, 4, null_attributes)\n",
    "    verified_fds = verify_FDs(df, candidates)\n",
    "    if verified_fds != []:\n",
    "        return verified_fds\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_null: Ersetzt Null-Value\n",
    "\n",
    "def replace_Null(df, fds):\n",
    "    for lhs, rhs in fds:\n",
    "        rhs_copy = rhs.copy()\n",
    "        lhs_copy = lhs.copy()\n",
    "        if len(rhs_copy) > 0:\n",
    "            rhs_col = rhs_copy.pop()\n",
    "            lhs_col = lhs_copy.pop()\n",
    "\n",
    "            # Step 1: Find index of row with specific rhs value\n",
    "            idx = df.index[df[rhs_col].isnull()]\n",
    "\n",
    "            # Step 2: Get corresponding lhs value(s)\n",
    "            lhs_values = df.loc[idx, lhs_col].tolist()\n",
    "\n",
    "            # Step 3: Group DataFrame by lhs\n",
    "            grouped = df.groupby(lhs_col)\n",
    "\n",
    "            # Step 4: Access group of rows with same lhs value as idx\n",
    "            for lhs_value in lhs_values:\n",
    "                if pd.isna(lhs_value): # is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    group = grouped.get_group(lhs_value)\n",
    "\n",
    "                # Step 5: Get corresponding rhs value(s) of rows in group\n",
    "                rhs_values = group[rhs_col].tolist()\n",
    "\n",
    "                # Step 6: Get value from the Group with the same lhs\n",
    "                for v in rhs_values:\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    else:\n",
    "                        value = v\n",
    "                        for i in idx:\n",
    "                            if df.loc[i, lhs_col] == lhs_value:\n",
    "                                df.loc[i, rhs_col] = value\n",
    "                            else:\n",
    "                                continue \n",
    "                    break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacer: bestimmt unteranderem Replacement-Rate\n",
    "\n",
    "def replacer(df):\n",
    "    fds = find_FDs(df)\n",
    "    null_befor = df.isnull().sum().sum()\n",
    "    null_after_overall = null_befor\n",
    "\n",
    "    if fds == []:\n",
    "        return 'There are no Functional Dependencies.'\n",
    "    else:\n",
    "        replace_Null(df, fds)\n",
    "        null_after = df.isnull().sum().sum()\n",
    "        null_after_overall -= (null_befor-null_after)\n",
    "        if null_befor == 0:\n",
    "            replace_rate = 0\n",
    "        else:\n",
    "            replace_rate = round((1-(null_after / null_befor)) * 100, 2)\n",
    "    \n",
    "    return df, replace_rate, null_befor, null_after"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main: ersetzt andere Null-Values durch NaN\n",
    "\n",
    "def main(df, null_values):\n",
    "   df.replace(null_values, np.nan, inplace=True)\n",
    "   if df.isnull().any().any():\n",
    "      replacer2 = replacer(df)\n",
    "      return replacer2\n",
    "   else:\n",
    "      return \"There are no NULL-Values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_iterator: iterriert durch Datensets und gibt finale Ausgabe zurÃ¼ck\n",
    "\n",
    "def dataset_iterator(df_list, null_values):\n",
    "    replace_values = []\n",
    "    for i in range(len(df_list)):\n",
    "        df = df_list[i]\n",
    "        df_copy = df.copy()\n",
    "        rate = main(df_copy, null_values)\n",
    "        if isinstance(rate, str):\n",
    "            replace_values.append([i, rate])\n",
    "        else:\n",
    "            replace_values.append([i, rate[1], rate[2], rate[3]])\n",
    "    return(len(df_list), replace_values)\n",
    "\n",
    "\n",
    "null_values = ['None', '--', 'NaN', 'Null', 'NA', 'undefined', 'Inf', 'inf', 'NULL']\n",
    "df_list = create_df_list(\"C:/Users/ilove/Downloads/result_null_filtered/result_nan_filtered/Used Datasets/test\")\n",
    "dataset_iterator(df_list, null_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "null_values = ['None', '--', 'NaN', 'Null', 'NA', 'undefined', 'Inf', 'inf', 'NULL']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# df_list = greate_df_list(\"C:/Users/ilove/Downloads/result_null_filtered/result_nan_filtered/Neuer Ordner\")\n",
    "# evaluation = dataset_iterator(df_list, null_values, 3)\n",
    "\n",
    "rate_list = []\n",
    "counter_null = 0\n",
    "counter_FD = 0\n",
    "counter_rate = 0\n",
    "for j in range(len(evaluation[1])):\n",
    "    if len(evaluation[1][j]) == 2:\n",
    "        if evaluation[1][j][1] == 'There are no NULL-Values':\n",
    "            counter_null += 1\n",
    "        else:\n",
    "            counter_FD += 1\n",
    "    else:\n",
    "        counter_rate += 1\n",
    "        rate_list.append(evaluation[1][j])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "running_time = end_time - start_time\n",
    "\n",
    "count = [evaluation[0], counter_rate, counter_null, counter_FD, running_time]\n",
    "\n",
    "df_rate = pd.DataFrame(rate_list, columns=['ID', 'replacement rate', 'Null befor', 'Null after'])\n",
    "df_rate[\"Null difference\"] = df_rate[\"Null befor\"] - df_rate[\"Null after\"]\n",
    "#df_rate.to_csv('evaluation_Dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b67b6f31813b43a9eeca2c28552d779fb7ec114f56fa4614be1e42d3fab2337c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
