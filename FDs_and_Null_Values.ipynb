{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_list(path):\n",
    "    \n",
    "    # get a list of all CSV files in the directory\n",
    "    files = glob.glob(path + '/*.csv')\n",
    "\n",
    "    # create an empty list to store the DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # iterate over the list of files\n",
    "    for file in files:\n",
    "        # read in the CSV file using pandas\n",
    "        df = pd.read_csv(file)\n",
    "        # append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "    # returns the list of dataframes\n",
    "    return df_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastFD(df, null_attributes):\n",
    "    \"\"\"\n",
    "    Finds functional dependencies in a DataFrame using the FastFD algorithm.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the set of FDs with the trivial ones\n",
    "    fds = set([(frozenset([attr]), frozenset([])) for attr in df.columns])\n",
    "    non_trivial_fds = set()\n",
    "\n",
    "    # Step 2: Compute the equivalence classes of the tuples\n",
    "    eq_classes = compute_equivalence_classes(df)\n",
    "\n",
    "    # Step 3: Compute the closure of each attribute set\n",
    "    closures = {}\n",
    "    for attr in df.columns:\n",
    "        closures[frozenset([attr])] = closure(set([attr]), fds)\n",
    "\n",
    "    # Step 4: Initialize the set of candidate pairs\n",
    "    candidate_pairs = set([(frozenset([A]), frozenset([B])) for A in df.columns for B in list(null_attributes) if A != B])\n",
    "\n",
    "    # Step 5: Repeat until there are no more FDs to be found\n",
    "    while len(candidate_pairs) > 0:\n",
    "        # Step 5a: Choose a pair of attributes (A, B) such that A -> B is not already known\n",
    "        (A, B) = candidate_pairs.pop()\n",
    "\n",
    "        # Step 5b: Compute A+\n",
    "        AB_union = A.union(B)\n",
    "        AB_closure = closures[A].intersection(closures[B])\n",
    "        for attr in df.columns:\n",
    "            AB_attr = AB_union.union(set([attr]))\n",
    "            if AB_attr not in closures:\n",
    "                closures[AB_attr] = closure(AB_attr, fds)\n",
    "\n",
    "        # Step 5c: If B is in A+, add A -> B to the set of known FDs\n",
    "        if B not in AB_closure:\n",
    "            fds.add((A, B))\n",
    "            non_trivial_fds.add((A, B))\n",
    "\n",
    "            # Update the closures of all supersets of A\n",
    "            for attr_set in [s for s in closures.keys() if A.issubset(s) and s != A]:\n",
    "                closures[attr_set] = closures[attr_set].intersection(AB_closure)\n",
    "\n",
    "            # Update the set of candidate pairs\n",
    "            for C in null_attributes:\n",
    "                if C not in AB_union:\n",
    "                    candidate_pairs.add((AB_union, frozenset([C])))\n",
    "\n",
    "    # Step 6: Return the set of known FDs\n",
    "    result = [(list(X), list(Y)) for (X, Y) in fds if Y and len(X) <= 4]\n",
    "    return result\n",
    "\n",
    "def compute_equivalence_classes(df):\n",
    "    \"\"\"\n",
    "    Computes the equivalence classes of the tuples in the DataFrame.\n",
    "    \"\"\"\n",
    "    eq_classes = defaultdict(set)\n",
    "    for row in df.itertuples(index=False):\n",
    "        eq_classes[row] = set(row)\n",
    "    return eq_classes\n",
    "\n",
    "def closure(X, fds):\n",
    "    \"\"\"\n",
    "    Computes the closure of a set of attributes X given a set of FDs.\n",
    "    \"\"\"\n",
    "    X_closure = set(X)\n",
    "    while True:\n",
    "        changed = False\n",
    "        for (A, B) in fds:\n",
    "            if A.issubset(X_closure) and not B.issubset(X_closure):\n",
    "                X_closure = X_closure.union(B)\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "    return X_closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_FDs(df, candidates):\n",
    "    # create an empty list to store the verified functional dependencies\n",
    "    fds = []\n",
    "\n",
    "    # iterate over the list of candidate pairs of attributes\n",
    "    for lhs, rhs in candidates:\n",
    "        # group the DataFrame by the LHS attributes and check if the number of unique values in the RHS column is equal to 1\n",
    "        if (df.groupby(list(lhs))[list(rhs)].nunique().eq(1).all() == True).all():\n",
    "            # if the above condition is True for all groups, \n",
    "            # add the candidate pair to the list of verified functional dependencies\n",
    "            fds.append((set(lhs), set(rhs)))\n",
    "\n",
    "    # Remove any candidate pairs where there is another candidate pair with the same RHS and a larger LHS\n",
    "    # create an empty list to store the pruned functional dependencies\n",
    "    pruned_fds = []\n",
    "    for lhs1, rhs1 in fds:\n",
    "        # iterate over the list of verified functional dependencies and check if the RHS and LHS attributes of the current \n",
    "        # dependency are the same as those of another dependency, but the LHS of the other dependency is a proper subset of \n",
    "        # the LHS of the current dependency\n",
    "        is_superset = False\n",
    "        for lhs2, rhs2 in fds:\n",
    "            if lhs1.issuperset(lhs2) and rhs1 == rhs2 and lhs1 != lhs2:\n",
    "                is_superset = True\n",
    "                break\n",
    "        # if there is no other dependency with the same RHS and a larger LHS, add the current dependency to the list of pruned \n",
    "        # functional dependencies\n",
    "        if not is_superset:\n",
    "            pruned_fds.append((lhs1, rhs1))\n",
    "\n",
    "    # return the list of pruned functional dependencies\n",
    "    return pruned_fds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_FDs(df):\n",
    "    # Find columns with null values\n",
    "    null_attributes = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    # Generate candidate FDs using fastFD algorithm with the null attributes\n",
    "    candidates = fastFD(df, null_attributes)\n",
    "    \n",
    "    # Verify the candidate FDs using the verify_FDs function\n",
    "    verified_fds = verify_FDs(df, candidates)\n",
    "    \n",
    "    # If there are verified FDs, return them. Otherwise, return an empty list.\n",
    "    if verified_fds != []:\n",
    "        return verified_fds\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_Null(df, fds):\n",
    "    # Iterate through each functional dependency (FD) in the list of verified FDs\n",
    "    for lhs, rhs in fds:\n",
    "        # Create copies of the LHS and RHS sets to modify\n",
    "        rhs_copy = rhs.copy()\n",
    "        lhs_copy = lhs.copy()\n",
    "        \n",
    "        # Check if the RHS set is not empty\n",
    "        if len(rhs_copy) > 0:\n",
    "            # Pop the last column name from the RHS set and the corresponding LHS column name\n",
    "            rhs_col = rhs_copy.pop()\n",
    "            lhs_col = lhs_copy.pop()\n",
    "\n",
    "            # Step 1: Find index of rows with missing values in the RHS column\n",
    "            idx = df.index[df[rhs_col].isnull()]\n",
    "\n",
    "            # Step 2: Get corresponding LHS values for the rows with missing values in the RHS column\n",
    "            lhs_values = df.loc[idx, lhs_col].tolist()\n",
    "\n",
    "            # Step 3: Group the DataFrame by the LHS column\n",
    "            grouped = df.groupby(lhs_col)\n",
    "\n",
    "            # Step 4: Access the group of rows with the same LHS value(s) as the rows with missing values in the RHS column\n",
    "            for lhs_value in lhs_values:\n",
    "                # Check if the LHS value NaN\n",
    "                if pd.isna(lhs_value):\n",
    "                    continue\n",
    "                else:\n",
    "                    group = grouped.get_group(lhs_value)\n",
    "\n",
    "                # Step 5: Get corresponding RHS value(s) of the rows in the group\n",
    "                rhs_values = group[rhs_col].tolist()\n",
    "\n",
    "                # Step 6: Replace the missing values in the RHS column with the corresponding value from the group with the same LHS value\n",
    "                for v in rhs_values:\n",
    "                    # Check if the RHS value is None or NaN\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    else:\n",
    "                        value = v\n",
    "                        for i in idx:\n",
    "                            if df.loc[i, lhs_col] == lhs_value:\n",
    "                                df.loc[i, rhs_col] = value\n",
    "                            else:\n",
    "                                continue \n",
    "                    break\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(df):\n",
    "    # find the Functional Dependencies in the dataset\n",
    "    fds = find_FDs(df)\n",
    "    \n",
    "    # count the number of null values before the replacement\n",
    "    null_befor = df.isnull().sum().sum()\n",
    "\n",
    "    # if there are no Functional Dependencies, return a message saying so\n",
    "    if fds == []:\n",
    "        return 'There are no Functional Dependencies.'\n",
    "    else:\n",
    "        # replace the null values in the dataset using the Functional Dependencies\n",
    "        replace_Null(df, fds)\n",
    "        \n",
    "        # count the number of null values after the replacement\n",
    "        null_after = df.isnull().sum().sum()\n",
    "        \n",
    "        # calculate the Replacement Rate as percentage of null values that were replaced\n",
    "        if null_befor == 0:\n",
    "            replace_rate = 0\n",
    "        else:\n",
    "            replace_rate = round((1-(null_after / null_befor)) * 100, 2)\n",
    "    \n",
    "    # return the updated dataset, the Replacement Rate, \n",
    "    # the initial number of null values, and the final number of null values\n",
    "    return df, replace_rate, null_befor, null_after\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, null_values):\n",
    "    # replaces all values within the table that are in null-values witht NaN\n",
    "    df.replace(null_values, np.nan, inplace=True)\n",
    "\n",
    "    # Check if there are any null values in the DataFrame\n",
    "    if df.isnull().any().any():\n",
    "        # Call the replacer function to replace null values with FDs\n",
    "        replacer2 = replacer(df)\n",
    "        return replacer2\n",
    "    else:\n",
    "        return \"There are no NULL-Values\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_iterator(df_list, null_values):\n",
    "    replace_values = []\n",
    "    # Iterate over each dataframe in the list\n",
    "    for i in range(len(df_list)):\n",
    "        df = df_list[i]\n",
    "        # Call the main function to replace null values\n",
    "        rate = main(df, null_values)\n",
    "        # If there are no null values in the dataframe\n",
    "        if isinstance(rate, str):\n",
    "            # Add a string indicating there were no null values\n",
    "            replace_values.append([i, rate])\n",
    "        # If there were null values in the dataframe\n",
    "        else:\n",
    "            # Add the index of the dataframe and the replacement rate to the list\n",
    "            replace_values.append([i, rate[1], rate[2], rate[3]])\n",
    "    # Return the total number of dataframes and the list of replacement values\n",
    "    return(len(df_list), replace_values)\n",
    "\n",
    "null_values = ['None', '--', 'NaN', 'Null', 'NA', 'undefined', 'Inf', 'inf', 'NULL']\n",
    "df_list = create_df_list(\"C:/Users/ilove/OneDrive/Uni/Master - Philipps Uni/3. Semester/Seminar - Lakehouse/Code/FDs_and_Null-Values/Used Datasets\")\n",
    "dataset_iterator(df_list, null_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation porpus only\n",
    "\n",
    "# Define a list of possible null values\n",
    "null_values = ['None', '--', 'NaN', 'Null', 'NA', 'undefined', 'Inf', 'inf', 'NULL']\n",
    "\n",
    "# Create a list of dataframes from a directory and perform data cleaning on each dataframe\n",
    "# df_list = greate_df_list(\"C:/Users/ilove/Downloads/result_null_filtered/result_nan_filtered/Neuer Ordner\")\n",
    "# evaluation = dataset_iterator(df_list, null_values, 3)\n",
    "\n",
    "# Initialize counters and a list to store the replacement rate for each dataframe\n",
    "rate_list = []\n",
    "counter_null = 0\n",
    "counter_FD = 0\n",
    "counter_rate = 0\n",
    "\n",
    "# Iterate over the evaluation results for each dataframe\n",
    "for j in range(len(evaluation[1])):\n",
    "    if len(evaluation[1][j]) == 2:\n",
    "        # If there are no null values in the dataframe, increment the counter\n",
    "        if evaluation[1][j][1] == 'There are no NULL-Values':\n",
    "            counter_null += 1\n",
    "        # If there are FDs and null values, increment the counter and store the replacement rate\n",
    "        else:\n",
    "            counter_FD += 1\n",
    "    else:\n",
    "        # If there are null values but no FDs, increment the counter and store the replacement rate\n",
    "        counter_rate += 1\n",
    "        rate_list.append(evaluation[1][j])\n",
    "\n",
    "\n",
    "count = [evaluation[0], counter_rate, counter_null, counter_FD, running_time]\n",
    "\n",
    "# Create a pandas dataframe to store the replacement rates for each dataframe\n",
    "df_rate = pd.DataFrame(rate_list, columns=['ID', 'replacement rate', 'Null befor', 'Null after'])\n",
    "df_rate[\"Null difference\"] = df_rate[\"Null befor\"] - df_rate[\"Null after\"]\n",
    "\n",
    "# Export the results to a CSV file\n",
    "# df_rate.to_csv('evaluation_Dataset.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b67b6f31813b43a9eeca2c28552d779fb7ec114f56fa4614be1e42d3fab2337c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
